\chapter{Introduction}

Modern particle physics is becoming increasingly dependent on Machine Learning (ML) for essential analytical tasks, ranging from event reconstruction to particle identification. A notable application of this is jet flavour tagging, where algorithms aim to distinguish jets that originate from different quark flavours. The identification of bottom-quark jets plays a central role in many precision measurements and searches for new physics at the Large Hadron Collider (LHC). However, the robustness of these models is critical. In the broader ML community, adversarial attacks — small, deliberately designed perturbations that can mislead a model — have been widely studied, particularly in computer vision. However, their potential impact in particle physics remains under-explored, particularly in scenarios involving discrete-valued input features.

Jet-tagging algorithms such as DeepJet process hundreds of features, including continuous detector observables and discrete quantities (e.g. counts and category identifiers). Most adversarial studies treat all inputs as continuous, overlooking the unique behaviour and vulnerabilities of models when discrete inputs are perturbed. This gap offers an opportunity to probe robustness in a more comprehensive way.

This thesis addresses this challenge by introducing Probabilistic Integer Perturbation (PIP), an adversarial method tailored to discrete features. PIP uses gradient information to assign feature-specific probabilities for discrete changes, ensuring that all perturbations remain physically meaningful. This method is then combined with the Projected Gradient Descent (PGD) attack, targeting both continuous and discrete domains simultaneously and providing a broader evaluation of model vulnerabilities.

This study utilises the DeepJet architecture, which has been trained on approximately 10 million simulated Compact Muon Solenoid (CMS) jets across all major flavour categories. The findings indicate that, while DeepJet exhibits relative resilience to standard continuous attacks, it can be substantially impacted by discrete perturbations. The combined Probabilistic Integer-Perturbed Projected Gradient Descent (PIP-PGD) attack amplifies this effect, while adversarial training with both methods yields the most balanced robustness across attack types.

Although discrete perturbations do not directly correspond to realistic detector effects, they serve as a diagnostic tool to reveal model dependencies on discretised inputs. Insights that remain hidden in continuous-only studies are revealed through the use of these perturbations. The work presented here contributes to the development of a more complete understanding of adversarial robustness in HEP machine learning and outlines approaches for developing models that are both accurate and resilient in demanding scientific applications.

